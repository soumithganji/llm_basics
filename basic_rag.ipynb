{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sEUbucXkPuiU"
      },
      "outputs": [],
      "source": [
        "!pip install pypdf\n",
        "!pip install -q transformers einops accelerate langchain bitsandbytes\n",
        "!pip install sentence_transformers\n",
        "!pip install llama_index\n",
        "!pip install llama-index-llms-huggingface\n",
        "!pip install -U langchain-community\n",
        "!pip install llama-index-embeddings-langchain\n",
        "!pip install transformers --upgrade\n",
        "!pip install langchain --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "CztKPagdQ8mB"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import VectorStoreIndex,SimpleDirectoryReader,ServiceContext\n",
        "from llama_index.llms.huggingface import HuggingFaceLLM\n",
        "from llama_index.core.prompts.prompts import SimpleInputPrompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l8fxXK2HLV9O"
      },
      "outputs": [],
      "source": [
        "documents = SimpleDirectoryReader(input_dir='./data').load_data()\n",
        "documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "nZb9miCSLkWT"
      },
      "outputs": [],
      "source": [
        "system_prompt=\"\"\"\n",
        "You are a Q&A assistant. Your goal is to answer questions as\n",
        "accurately as possible based on the instructions and context provided.\n",
        "\"\"\"\n",
        "\n",
        "query_wrapper_prompt=SimpleInputPrompt(\"<|USER|>{query_str}<|ASSISTANT|>\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "DAvWGbTZNZi0"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "login(token =\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "69sg778ANdao"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"daryl149/llama-2-7b-chat-hf\")\n",
        "tokenizer.chat_template = \"<|user|>{user_message}<|assistant|>{assistant_message}\"\n",
        "\n",
        "llm = HuggingFaceLLM(\n",
        "    context_window=4096,\n",
        "    max_new_tokens=256,\n",
        "    generate_kwargs={\"temperature\": 0.0, \"do_sample\": False},\n",
        "    system_prompt=system_prompt,\n",
        "    query_wrapper_prompt=query_wrapper_prompt,\n",
        "    tokenizer=tokenizer,\n",
        "    model_name=\"daryl149/llama-2-7b-chat-hf\",\n",
        "    device_map=\"auto\",\n",
        "    model_kwargs={\"torch_dtype\": torch.float16 , \"load_in_8bit\":True}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "-Pbk_Ux0S-H5"
      },
      "outputs": [],
      "source": [
        "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
        "from llama_index.core import Settings\n",
        "from llama_index.embeddings.langchain import LangchainEmbedding\n",
        "\n",
        "embed_model = LangchainEmbedding(HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "9me_uzWlOWtJ"
      },
      "outputs": [],
      "source": [
        "Settings.llm = llm\n",
        "Settings.embed_model = embed_model\n",
        "Settings.chunk_size = 1024"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "rZd_lYMLQxI-"
      },
      "outputs": [],
      "source": [
        "index = VectorStoreIndex.from_documents(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M19Qk4vN1Ygr"
      },
      "outputs": [],
      "source": [
        "query_engine = index.as_chat_engine()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0IPaOBMz10DL"
      },
      "outputs": [],
      "source": [
        "user_message = \"What is Soumith's latest education?\"\n",
        "response = query_engine.query(user_message)\n",
        "print(response.response)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}